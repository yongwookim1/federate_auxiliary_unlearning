{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"IuuApPD02fhH","trusted":true},"outputs":[],"source":["import os\n","from tqdm import tqdm\n","import random\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9PfGEVyU4XAM","trusted":true},"outputs":[],"source":["learning_rate = 1e-2\n","batch_size = 32\n","num_epochs = 2"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"tGaSJjE8267w","outputId":"4e1e2b51-9de2-4181-e242-34f312104795","trusted":true},"outputs":[],"source":["def create_dataloaders(batch_size):\n","    transform = transforms.Compose([\n","        transforms.Resize((32, 32)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485],\n","                            std=[0.228]),\n","    ])\n","\n","    train_data = datasets.MNIST(root=\"../data/\", train=True, download=True, transform=transform)\n","    test_data = datasets.MNIST(root=\"../data/\", train=False,download=True, transform=transform)\n","\n","    train_dataloader = DataLoader(train_data,\n","                                batch_size=batch_size,\n","                                num_workers=os.cpu_count(),\n","                                shuffle=True)\n","    test_dataloader = DataLoader(test_data,\n","                                batch_size=batch_size,\n","                                num_workers=os.cpu_count(),\n","                                shuffle=False)\n","    return train_dataloader, test_dataloader"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7z_JLHqO3gNa","trusted":true},"outputs":[],"source":["class LeNet5(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(LeNet5,self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(1, 6, kernel_size=5, stride=1),\n","            nn.Tanh(),\n","            nn.AvgPool2d(2, 2),\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(6, 16, kernel_size=5, stride=1),\n","            nn.Tanh(),\n","            nn.AvgPool2d(2, 2),\n","        )\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(16, 120, kernel_size=5, stride=1),\n","            nn.Tanh(),\n","        )\n","        self.fc1 = nn.Linear(120, 100)\n","        self.fc2 = nn.Linear(100, 1000)\n","        self.fc3 = nn.Linear(1000, num_classes)\n","        self.tanh = nn.Tanh()\n","        self.flatten = nn.Flatten()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.tanh(x)\n","        x = self.fc2(x)\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["### Training learning module"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"AekziRwN4ai9","trusted":true},"outputs":[],"source":["def train_step(model, dataloader, loss_fn, optimizer, device):\n","    epoch_loss = 0.0\n","    epoch_acc = 0.0\n","    model.train()\n","    for batch, (X, y) in enumerate(dataloader):\n","        X, y = X.to(device), y.to(device)\n","\n","        y_pred = model(X)\n","\n","        loss = loss_fn(y_pred, y)\n","        epoch_loss += loss.item()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        preds = torch.argmax(y_pred, dim=1)\n","        epoch_acc += torch.sum(preds == y) / len(y_pred)\n","\n","    epoch_loss = epoch_loss / len(dataloader)\n","    epoch_acc = epoch_acc / len(dataloader)\n","    return epoch_loss, epoch_acc\n","\n","\n","def test_step(model, dataloader, loss_fn, device):\n","    epoch_loss = 0.0\n","    epoch_acc = 0.0\n","    model.eval()\n","    classes_total = {f\"{i}\": 0 for i in range(10)}\n","    classes_wrong = {f\"{i}\": 0 for i in range(10)}\n","    with torch.inference_mode():\n","        for batch, (X, y) in enumerate(dataloader):\n","            X, y = X.to(device), y.to(device)\n","\n","            y_pred = model(X)\n","\n","            loss = loss_fn(y_pred, y)\n","            epoch_loss += loss.item()\n","\n","            preds = torch.argmax(y_pred, dim=1)\n","            for i in y:\n","                classes_total[f\"{i.item()}\"] += 1\n","            for i in y[y != preds]:\n","                classes_wrong[f\"{i.item()}\"] += 1\n","                \n","            epoch_acc += torch.sum(preds == y) / len(y_pred)\n","    \n","    epoch_loss = epoch_loss / len(dataloader)\n","    epoch_acc = epoch_acc / len(dataloader)\n","    return epoch_loss, epoch_acc, classes_wrong, classes_total\n","\n","\n","def train(model, train_dataloader, test_dataloader, loss_fn, optimizer, num_epochs, device):\n","    results = {\"train_loss\": [],\n","               \"test_loss\": [],\n","               \"train_acc\": [],\n","               \"test_acc\": []}\n","\n","    best_acc = 0.0\n","    for epoch in tqdm(range(1, num_epochs+1)):\n","        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n","        test_loss, test_acc, classes_wrong, classes_total = test_step(model, test_dataloader, loss_fn, device)\n","        \n","        if test_acc >= best_acc:\n","            torch.save(model.state_dict(), './weights/model.pth')\n","            best_acc = test_acc\n","\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        results[\"test_loss\"].append(test_loss)\n","        results[\"test_acc\"].append(test_acc)\n","\n","        print(f\"Epoch: {epoch} | \"\n","              f\"Train Loss: {train_loss:.4f} | \"\n","              f\"Train Acc: {train_acc:.4f} | \"\n","              f\"Test Loss: {test_loss:.4f} | \"\n","              f\"Test Acc: {test_acc:.4f}\")\n","        \n","        for c in classes_wrong.keys():\n","            print(f\"class {c}: {((classes_total[c]-classes_wrong[c])/classes_total[c] * 100):.2f}%\")\n","    return results, classes_wrong, classes_total"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"bPc27XVj7y6n","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda\n","GPU number: 0\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 1/2 [00:20<00:20, 20.91s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | Train Loss: 0.6861 | Train Acc: 0.8025 | Test Loss: 0.2667 | Test Acc: 0.9227\n","class 0: 98.16%\n","class 1: 97.89%\n","class 2: 88.95%\n","class 3: 90.89%\n","class 4: 93.38%\n","class 5: 86.88%\n","class 6: 93.84%\n","class 7: 91.34%\n","class 8: 89.84%\n","class 9: 90.49%\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:41<00:00, 20.88s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 | Train Loss: 0.2198 | Train Acc: 0.9352 | Test Loss: 0.1540 | Test Acc: 0.9546\n","class 0: 98.78%\n","class 1: 98.85%\n","class 2: 94.86%\n","class 3: 94.85%\n","class 4: 95.21%\n","class 5: 92.49%\n","class 6: 97.29%\n","class 7: 93.09%\n","class 8: 94.76%\n","class 9: 93.86%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Device: {device}\")\n","if device == \"cuda\":\n","    print(f\"GPU number: {torch.cuda.current_device()}\")\n","\n","model = LeNet5(num_classes=10).to(device)\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=4e-5)\n","\n","set_seed(42)\n","\n","train_dataloader, test_dataloader = create_dataloaders(batch_size=batch_size)\n","\n","results = train(model, train_dataloader, test_dataloader, loss_fn, optimizer, num_epochs, device)"]},{"cell_type":"markdown","metadata":{},"source":["### Training auxiliary unlearning module"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["313it [00:01, 190.65it/s]00<?, ?it/s]\n"," 33%|███▎      | 1/3 [00:21<00:42, 21.15s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | Train Loss: 0.4514 | Train Acc: 0.8729 | Test Loss: 0.3655 | Test Acc: 0.8722\n","class 0: 98.57%\n","class 1: 98.85%\n","class 2: 96.03%\n","class 3: 1.29%\n","class 4: 97.05%\n","class 5: 98.43%\n","class 6: 96.97%\n","class 7: 95.62%\n","class 8: 96.61%\n","class 9: 93.76%\n"]},{"name":"stderr","output_type":"stream","text":["313it [00:01, 199.95it/s]\n"," 67%|██████▋   | 2/3 [00:41<00:20, 20.78s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 | Train Loss: 0.3727 | Train Acc: 0.8826 | Test Loss: 0.3218 | Test Acc: 0.8788\n","class 0: 99.08%\n","class 1: 99.03%\n","class 2: 97.38%\n","class 3: 1.68%\n","class 4: 98.37%\n","class 5: 97.42%\n","class 6: 97.60%\n","class 7: 97.76%\n","class 8: 96.00%\n","class 9: 95.24%\n"]},{"name":"stderr","output_type":"stream","text":["313it [00:01, 181.76it/s]\n","100%|██████████| 3/3 [01:03<00:00, 21.11s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3 | Train Loss: 0.3504 | Train Acc: 0.8873 | Test Loss: 0.2983 | Test Acc: 0.8829\n","class 0: 98.98%\n","class 1: 99.12%\n","class 2: 98.35%\n","class 3: 1.68%\n","class 4: 97.86%\n","class 5: 98.21%\n","class 6: 98.54%\n","class 7: 96.69%\n","class 8: 99.08%\n","class 9: 95.34%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["def train_step(model, dataloader, loss_fn, optimizer, device):\n","    epoch_loss = 0.0\n","    epoch_acc = 0.0\n","    model.train()\n","    for batch, (X, y) in enumerate(dataloader):\n","        X, y = X.to(device), y.to(device)\n","        y[y==3] = torch.randint(0, 10, size=[1]).to(device)\n","        \n","        y_pred = model(X)\n","        \n","        loss = loss_fn(y_pred, y)\n","        epoch_loss += loss.item()\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        preds = torch.argmax(y_pred, dim=1)\n","        epoch_acc += torch.sum(preds == y) / len(y_pred)\n","\n","    epoch_loss = epoch_loss / len(dataloader)\n","    epoch_acc = epoch_acc / len(dataloader)\n","    return epoch_loss, epoch_acc\n","\n","\n","def test_step(model, dataloader, loss_fn, device):\n","    epoch_loss = 0.0\n","    epoch_acc = 0.0\n","    model.eval()\n","    \n","    classes_wrong = {f\"{i}\": 0 for i in range(10)}\n","    classes_total = {f\"{i}\": 0 for i in range(10)}\n","    with torch.inference_mode():\n","        for batch, (X, y) in tqdm(enumerate(dataloader)):\n","            X, y = X.to(device), y.to(device)\n","\n","            y_pred = model(X)\n","\n","            loss = loss_fn(y_pred, y)\n","            epoch_loss += loss.item()\n","\n","            preds = torch.argmax(y_pred, dim=1)\n","            for i in y:\n","                classes_total[f\"{i.item()}\"] += 1\n","            for i in y[y != preds]:\n","                classes_wrong[f\"{i.item()}\"] += 1\n","                \n","            epoch_acc += torch.sum(preds == y) / len(y_pred)\n","    \n","    epoch_loss = epoch_loss / len(dataloader)\n","    epoch_acc = epoch_acc / len(dataloader)\n","    return epoch_loss, epoch_acc, classes_wrong, classes_total\n","\n","\n","def train(model, train_dataloader, test_dataloader, loss_fn, optimizer, num_epochs, device):\n","    results = {\"train_loss\": [],\n","               \"test_loss\": [],\n","               \"train_acc\": [],\n","               \"test_acc\": []}\n","\n","    for epoch in tqdm(range(1, num_epochs+1)):\n","        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n","        test_loss, test_acc, classes_wrong, classes_total = test_step(model, test_dataloader, loss_fn, device)\n","        \n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        results[\"test_loss\"].append(test_loss)\n","        results[\"test_acc\"].append(test_acc)\n","\n","        print(f\"Epoch: {epoch} | \"\n","              f\"Train Loss: {train_loss:.4f} | \"\n","              f\"Train Acc: {train_acc:.4f} | \"\n","              f\"Test Loss: {test_loss:.4f} | \"\n","              f\"Test Acc: {test_acc:.4f}\")\n","    \n","        for c in classes_wrong.keys():\n","            print(f\"class {c}: {((classes_total[c]-classes_wrong[c])/classes_total[c] * 100):.2f}%\")\n","    torch.save(model.state_dict(), './weights/model_s.pth')\n","    return results, classes_wrong, classes_total\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","model = LeNet5(num_classes=10).to(device)\n","model.load_state_dict(torch.load(\"./weights/model.pth\", weights_only=True))\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=4e-5)\n","\n","train_dataloader, _ = create_dataloaders(batch_size=32)\n","\n","set_seed(42)\n","\n","results, classes_wrong, classes_total = train(model, train_dataloader, test_dataloader, loss_fn, optimizer, num_epochs=3, device=device)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["313it [00:01, 191.10it/s]"]},{"name":"stdout","output_type":"stream","text":["class 0: 98.16%\n","class 1: 98.59%\n","class 2: 96.03%\n","class 3: 69.70%\n","class 4: 95.11%\n","class 5: 95.52%\n","class 6: 96.66%\n","class 7: 92.32%\n","class 8: 95.38%\n","class 9: 94.65%\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model = LeNet5(num_classes=10).to(device)\n","model.load_state_dict(torch.load(\"./weights/model.pth\", weights_only=True))\n","\n","model_s = LeNet5(num_classes=10).to(device)\n","model_s.load_state_dict(torch.load(\"./weights/model_s.pth\", weights_only=True))\n","\n","A = 0.01\n","Wl0 = (A * model.fc2.weight.data) + ((1-A) * model_s.fc2.weight.data)\n","model.fc2.weight.data = Wl0\n","Wl1 = (A * model.fc3.weight.data) + ((1-A) * model_s.fc3.weight.data)\n","model.fc3.weight.data = Wl1\n","\n","_, dataloader = create_dataloaders(batch_size=32)\n","\n","epoch_loss, epoch_acc, classes_wrong, classes_total = test_step(model, dataloader, loss_fn, device)\n","\n","for c in classes_wrong.keys():\n","    print(f\"class {c}: {((classes_total[c]-classes_wrong[c])/classes_total[c] * 100):.2f}%\")"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":4}
